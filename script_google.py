import requests
import os
import pandas as pd
import time

# âœ… Load API Keys
GOOGLE_PLACES_API_KEY = os.getenv("GOOGLE_PLACES_API_KEY")
DATAFORSEO_USERNAME = os.getenv("DATAFORSEO_USERNAME")
DATAFORSEO_PASSWORD = os.getenv("DATAFORSEO_PASSWORD")

# âœ… Get Google Place ID
def get_place_id(business_name):
    """Fetches Place ID from Google Places API based on business name."""
    print(f"ğŸ” Searching Google Places API for: {business_name}")

    url = "https://maps.googleapis.com/maps/api/place/findplacefromtext/json"
    params = {
        "input": business_name,
        "inputtype": "textquery",
        "fields": "place_id,formatted_address",
        "key": GOOGLE_PLACES_API_KEY
    }

    response = requests.get(url, params=params)
    data = response.json()
    print(f"ğŸ” Google Places Response: {data}")  # Debugging

    if data.get("status") == "OK" and data.get("candidates"):
        place_id = data["candidates"][0]["place_id"]
        address = data["candidates"][0]["formatted_address"]
        print(f"âœ… Found Place ID: {place_id} for {business_name} ({address})")
        return place_id
    else:
        print("âŒ Error extracting Place ID: No result found in API response.")
        return None

# âœ… Submit Review Task to DataForSEO
def submit_review_task(place_id, include_ratings, keywords, page_token=None):
    """Submits a request to DataForSEO to fetch Google reviews asynchronously."""
    print(f"ğŸ“¡ Submitting task for Place ID: {place_id} to DataForSEO")

    url = "https://api.dataforseo.com/v3/business_data/google/reviews/task_post"
    auth = (DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD)

    # âœ… Handle filtering properly
    filters = []
    if include_ratings and include_ratings.strip():  # âœ… Only add if not empty
        filters.append({"rating": include_ratings.split(",")})
    if keywords and keywords.strip():  # âœ… Only add if not empty
        filters.append({"text": keywords.split(",")})

    payload = [{
        "se": "google",
        "se_type": "reviews",
        "place_id": place_id,
        "reviews_limit": 2000,  # Fetch up to 2000 reviews
        "max_crawl_pages": 10,  # âœ… Fetch up to 10 pages
        "depth": 700,  # âœ… Max depth for parsing reviews
        "filters": filters,  # âœ… Filters only applied if not empty
        "language_code": "de",
        "location_name": "Germany",
        "device": "desktop",
        "os": "windows"
    }]

    if page_token:
        payload[0]["page_token"] = page_token  # âœ… Request next page if available

    headers = {"Content-Type": "application/json"}
    response = requests.post(url, auth=auth, json=payload, headers=headers)
    data = response.json()

    print(f"ğŸ“¡ DataForSEO Task Response: {data}")  # Debugging

    if "tasks" not in data or not data["tasks"]:
        print("âŒ DataForSEO API returned an empty response.")
        return None

    task = data["tasks"][0]
    if task.get("status_code") != 20100:  # âœ… 20100 means "Task Created"
        print(f"âŒ DataForSEO Error: {task.get('status_message')}")
        return None

    task_id = task["id"]
    print(f"âœ… Task Created: {task_id}")
    return task_id

# âœ… Fetch Completed Task Results with Pagination
def fetch_review_results(task_id):
    """Fetches completed review results from DataForSEO, handling pagination properly."""
    print(f"â³ Waiting for DataForSEO to process task: {task_id}")

    url = f"https://api.dataforseo.com/v3/business_data/google/reviews/task_get/{task_id}"
    auth = (DATAFORSEO_USERNAME, DATAFORSEO_PASSWORD)

    reviews = []
    next_page_token = None  # âœ… Start with None, but update dynamically

    while True:  # âœ… Loop indefinitely until all pages are scraped
        time.sleep(5)  # âœ… Delay between API calls

        response = requests.get(url, auth=auth)
        data = response.json()
        print(f"ğŸ“¡ DataForSEO Task Result: {data}")  # Debugging

        if "tasks" not in data or not data["tasks"]:
            print("âŒ No valid tasks found.")
            return None

        task = data["tasks"][0]
        if task.get("status_code") == 20000 and task.get("result"):
            result = task["result"][0]
            reviews.extend(result.get("items", []))  # âœ… Append reviews

            print(f"âœ… Scraped {len(reviews)} reviews so far...")

            next_page_token = result.get("next_page_token")  # âœ… Update token for next page

            if not next_page_token:  # âœ… Stop when there are no more pages
                break

            print(f"ğŸ”„ Fetching next page of reviews (Token: {next_page_token})")
            task_id = submit_review_task(result["place_id"], "", "", next_page_token)  # âœ… Fetch next page
            if not task_id:
                break  # âœ… Stop if no new task was created

        else:
            print(f"â³ Task not ready yet, retrying...")
            time.sleep(3)  # âœ… Add small delay before retrying

    if not reviews:
        print("âŒ Task timed out. No reviews found.")
        return None

    return reviews

# âœ… Get Google Reviews (Handles Pagination)
def get_google_reviews(business_name, include_ratings="", keywords=""):
    """Fetches Google Reviews from DataForSEO asynchronously, handling pagination."""

    # âœ… Step 1: Get Place ID
    place_id = get_place_id(business_name)
    if not place_id:
        print("âŒ No valid Place ID found.")
        return None  # Stop execution if no Place ID found

    # âœ… Step 2: Submit Task
    task_id = submit_review_task(place_id, include_ratings, keywords)
    if not task_id:
        return None  # Stop if task submission failed

    # âœ… Step 3: Fetch Task Results (All Pages)
    reviews = fetch_review_results(task_id)
    if not reviews:
        print("âŒ No reviews found.")
        return None

    # âœ… Save reviews to Excel
    filename = "google_reviews.xlsx"
    df = pd.DataFrame(reviews)
    df.to_excel(filename, index=False)
    
    print(f"âœ… Successfully saved {len(reviews)} reviews to {filename}")
    return filename